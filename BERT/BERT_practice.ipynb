{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT 구현 및 nlp \n",
    " 1. 데이터 전처리 및 BERT model pretraining\n",
    " 2. Naver 영화리뷰 감정분석 분류 모델로 BERT 사용해서 분류하기  \n",
    "   \n",
    "**Referenced by: https://paul-hyun.github.io/bert-01/**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sentencepiece as spm\n",
    "\n",
    "\"\"\" configuration json을 읽어들이는 class \"\"\"\n",
    "class Config(dict): \n",
    "    __getattr__ = dict.__getitem__\n",
    "    __setattr__ = dict.__setitem__\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, file):\n",
    "        with open(file, 'r') as f:\n",
    "            config = json.loads(f.read())\n",
    "            return Config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocab loading\n",
    "# 미리 만든 vocab 로드\n",
    "vocab_file = \"/opt/workspace/Seohyeon/Projects/LMMs/data_preprocessing/kowiki.model\"\n",
    "vocab = spm.SentencePieceProcessor()\n",
    "vocab.load(vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_enc_vocab': 8007, 'n_enc_seq': 256, 'n_seg_type': 2, 'n_layer': 6, 'd_hidn': 256, 'i_pad': 0, 'd_ff': 1024, 'n_head': 4, 'd_head': 64, 'dropout': 0.1, 'layer_norm_epsilon': 1e-12}\n"
     ]
    }
   ],
   "source": [
    "config = Config({\n",
    "    \"n_enc_vocab\": len(vocab),\n",
    "    \"n_enc_seq\": 256,\n",
    "    \"n_seg_type\": 2,\n",
    "    \"n_layer\": 6,\n",
    "    \"d_hidn\": 256,\n",
    "    \"i_pad\": 0,\n",
    "    \"d_ff\": 1024,\n",
    "    \"n_head\": 4,\n",
    "    \"d_head\": 64,\n",
    "    \"dropout\": 0.1,\n",
    "    \"layer_norm_epsilon\": 1e-12\n",
    "})\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" attention pad mask \"\"\"\n",
    "def get_attn_pad_mask(seq_q, seq_k, i_pad):\n",
    "    batch_size, len_q = seq_q.size()\n",
    "    batch_size, len_k = seq_k.size()\n",
    "    pad_attn_mask = seq_k.data.eq(i_pad)\n",
    "    pad_attn_mask= pad_attn_mask.unsqueeze(1).expand(batch_size, len_q, len_k)\n",
    "    return pad_attn_mask\n",
    "\n",
    "\"\"\" attention decoder mask \"\"\"\n",
    "def get_attn_decoder_mask(seq):\n",
    "    subsequent_mask = torch.ones_like(seq).unsqueeze(-1).expand(seq.size(0), seq.size(1), seq.size(1))\n",
    "    subsequent_mask = subsequent_mask.triu(diagonal=1) # upper triangular part of a matrix(2-D)\n",
    "    return subsequent_mask\n",
    "\n",
    "\"\"\" scale dot product attention \"\"\"\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.scale = 1 / (self.config.d_head ** 0.5)\n",
    "    \n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        # (bs, n_head, n_q_seq, n_k_seq)\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2))\n",
    "        scores = scores.mul_(self.scale)\n",
    "        scores.masked_fill_(attn_mask, -1e9)\n",
    "        # (bs, n_head, n_q_seq, n_k_seq)\n",
    "        attn_prob = nn.Softmax(dim=-1)(scores)\n",
    "        attn_prob = self.dropout(attn_prob)\n",
    "        # (bs, n_head, n_q_seq, d_v)\n",
    "        context = torch.matmul(attn_prob, V)\n",
    "        # (bs, n_head, n_q_seq, d_v), (bs, n_head, n_q_seq, n_v_seq)\n",
    "        return context, attn_prob\n",
    "    \n",
    "    \"\"\" multi head attention \"\"\"\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.W_Q = nn.Linear(self.config.d_hidn, self.config.n_head * self.config.d_head)\n",
    "        self.W_K = nn.Linear(self.config.d_hidn, self.config.n_head * self.config.d_head)\n",
    "        self.W_V = nn.Linear(self.config.d_hidn, self.config.n_head * self.config.d_head)\n",
    "        self.scaled_dot_attn = ScaledDotProductAttention(self.config)\n",
    "        self.linear = nn.Linear(self.config.n_head * self.config.d_head, self.config.d_hidn)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "    \n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        batch_size = Q.size(0)\n",
    "        # (bs, n_head, n_q_seq, d_head)\n",
    "        q_s = self.W_Q(Q).view(batch_size, -1, self.config.n_head, self.config.d_head).transpose(1,2)\n",
    "        # (bs, n_head, n_k_seq, d_head)\n",
    "        k_s = self.W_K(K).view(batch_size, -1, self.config.n_head, self.config.d_head).transpose(1,2)\n",
    "        # (bs, n_head, n_v_seq, d_head)\n",
    "        v_s = self.W_V(V).view(batch_size, -1, self.config.n_head, self.config.d_head).transpose(1,2)\n",
    "\n",
    "        # (bs, n_head, n_q_seq, n_k_seq)\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, self.config.n_head, 1, 1)\n",
    "\n",
    "        # (bs, n_head, n_q_seq, d_head), (bs, n_head, n_q_seq, n_k_seq)\n",
    "        context, attn_prob = self.scaled_dot_attn(q_s, k_s, v_s, attn_mask)\n",
    "        # (bs, n_head, n_q_seq, h_head * d_head)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.config.n_head * self.config.d_head)\n",
    "        # (bs, n_head, n_q_seq, e_embd)\n",
    "        output = self.linear(context)\n",
    "        output = self.dropout(output)\n",
    "        # (bs, n_q_seq, d_hidn), (bs, n_head, n_q_seq, n_k_seq)\n",
    "        return output, attn_prob\n",
    "    \n",
    "    \"\"\" feed forward \"\"\"\n",
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels=self.config.d_hidn, out_channels=self.config.d_ff, kernel_size=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=self.config.d_ff, out_channels=self.config.d_hidn, kernel_size=1)\n",
    "        self.active = F.gelu\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # (bs, d_ff, n_seq)\n",
    "        output = self.conv1(inputs.transpose(1, 2))\n",
    "        output = self.active(output)\n",
    "        # (bs, n_seq, d_hidn)\n",
    "        output = self.conv2(output).transpose(1, 2)\n",
    "        output = self.dropout(output)\n",
    "        # (bs, n_seq, d_hidn)\n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Encoder Layer\n",
    "- 표준 Transformer EncoderLayer와 동일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Encoder Layer \"\"\"\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.self_attn = MultiHeadAttention(self.config)\n",
    "        self.layer_norm1 = nn.LayerNorm(self.config.d_hidn, eps=self.config.layer_norm_epsilon)\n",
    "        self.pos_ffn = PoswiseFeedForwardNet(self.config)\n",
    "        self.layer_norm2 = nn.LayerNorm(self.config.d_hidn, eps=self.config.layer_norm_epsilon)\n",
    "    \n",
    "    def forward(self, inputs, attn_mask):\n",
    "        # (bs, n_enc_seq, d_hidn), (bs, n_head, n_enc_seq, n_enc_seq)\n",
    "        att_outputs, attn_prob = self.self_attn(inputs, inputs, inputs, attn_mask)\n",
    "        att_outputs = self.layer_norm1(inputs + att_outputs)\n",
    "        # (bs, n_enc_seq, d_hidn)\n",
    "        ffn_outputs = self.pos_ffn(att_outputs)\n",
    "        ffn_outputs = self.layer_norm2(ffn_outputs + att_outputs)\n",
    "        # (bs, n_enc_seq, d_hidn), (bs, n_head, n_enc_seq, n_enc_seq)\n",
    "        return ffn_outputs, attn_prob "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Encoder\n",
    "- 표준 Transformer Encoder와 아래가 다름\n",
    "    - 1) position을 학습 (line 8: freeze=False) \n",
    "    - 2) Segment Embedding 추가 (line 9)\n",
    "    - 3) Encoder input에 Segment 정보 추가 (line 13)\n",
    "    - 4) Token, Position 및 Segment 3가지 Embedding 더함 (line 19: 표준 Transformer에서는 Token, Position, 2가지 embedding 더함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" encoder \"\"\"\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.enc_emb = nn.Embedding(self.config.n_enc_vocab, self.config.d_hidn)\n",
    "        self.pos_emb = nn.Embedding(self.config.n_enc_seq + 1, self.config.d_hidn)\n",
    "        self.seg_emb = nn.Embedding(self.config.n_seg_type, self.config.d_hidn)\n",
    "\n",
    "        self.layers = nn.ModuleList([EncoderLayer(self.config) for _ in range(self.config.n_layer)])\n",
    "    \n",
    "    def forward(self, inputs, segments):\n",
    "        positions = torch.arange(inputs.size(1), device=inputs.device, dtype=inputs.dtype).expand(inputs.size(0), inputs.size(1)).contiguous() + 1\n",
    "        pos_mask = inputs.eq(self.config.i_pad)\n",
    "        positions.masked_fill_(pos_mask, 0)\n",
    "\n",
    "        # (bs, n_enc_seq, d_hidn)\n",
    "        outputs = self.enc_emb(inputs) + self.pos_emb(positions)  + self.seg_emb(segments)\n",
    "\n",
    "        # (bs, n_enc_seq, n_enc_seq)\n",
    "        attn_mask = get_attn_pad_mask(inputs, inputs, self.config.i_pad)\n",
    "\n",
    "        attn_probs = []\n",
    "        for layer in self.layers:\n",
    "            # (bs, n_enc_seq, d_hidn), (bs, n_head, n_enc_seq, n_enc_seq)\n",
    "            outputs, attn_prob = layer(outputs, attn_mask)\n",
    "            attn_probs.append(attn_prob)\n",
    "        # (bs, n_enc_seq, d_hidn), [(bs, n_head, n_enc_seq, n_enc_seq)]\n",
    "        return outputs, attn_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" bert \"\"\"\n",
    "class BERT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.encoder = Encoder(self.config)\n",
    "\n",
    "        self.linear = nn.Linear(config.d_hidn, config.d_hidn)\n",
    "        self.activation = torch.tanh\n",
    "    \n",
    "    def forward(self, inputs, segments):\n",
    "        # (bs, n_seq, d_hidn), [(bs, n_head, n_enc_seq, n_enc_seq)]\n",
    "        outputs, self_attn_probs = self.encoder(inputs, segments)\n",
    "        # (bs, d_hidn)\n",
    "        outputs_cls = outputs[:, 0].contiguous()\n",
    "        outputs_cls = self.linear(outputs_cls)\n",
    "        outputs_cls = self.activation(outputs_cls)\n",
    "        # (bs, n_enc_seq, n_enc_vocab), (bs, d_hidn), [(bs, n_head, n_enc_seq, n_enc_seq)]\n",
    "        \n",
    "        return outputs, outputs_cls, self_attn_probs\n",
    "    \n",
    "    def save(self, epoch, loss, path):\n",
    "        torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"loss\": loss,\n",
    "            \"state_dict\": self.state_dict()\n",
    "        }, path)\n",
    "    \n",
    "    def load(self, path):\n",
    "        save = torch.load(path)\n",
    "        self.load_state_dict(save[\"state_dict\"])\n",
    "        return save[\"epoch\"], save[\"loss\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT의 LOSS FUNCTION\n",
    "- MLM (Masked Language Model): mask된 부분의 단어 예측 \n",
    "    - 전체 단어의 15%를 선택 후, 그중 80%는 [mask], 10%는 현재 단어 유지, 10%는 임의의 단어로 대체\n",
    "- NSP (Next Sentence Prediction): 첫번째 [CLS] 토큰으로 문장 A와 B의 관계를 예측\n",
    "    - A 다음문장이 B가 맞을 경우는  True, A 다음문장이 B가 아닐 경우  False로 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT를 Pretrain 하기위한 클래스\n",
    "1. BERT의 결과를 입력으로 NSP를 예측하기위한 projection_cls 선언 (line: 9)\n",
    "2. BERT의 결과를 입력으로 MLM을 예측하기위한 projection_lm을 선언 (line: 11)\n",
    "3. projection_lm은 Encoder의 Embedding과 weight를 share함 (line: 12)\n",
    "4. inputs, segments를 입력으로 BERT 실행 (line: 16)\n",
    "5. outputs_cls를 입력으로 projection_cls를 실행해서 NSP를 예측하도록 함 (line: 18)\n",
    "6. outputs를 입력으로 projection_lm을 실행해서 MLM 예측하도록 함 (line: 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"BERT pretrain\"\"\"\n",
    "class BERTPretrain(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.bert = BERT(self.config)\n",
    "        # classifier\n",
    "        self.projection_cls = nn.Linear(self.config.d_hidn, 2, bias=False) # NSP (next sentence prediction)을 위한 classifier\n",
    "        # LM\n",
    "        self.projection_lm = nn.Linear(self.config.d_hidn, self.config.n_enc_vocab, bias=False) # MLM (Masked language model)을 위한 classifier\n",
    "        self.projection_lm.weight = self.bert.encoder.enc_emb.weight # 인코더의 임베딩과 weight를 공유\n",
    "    \n",
    "    def forward(self, inputs, segments):\n",
    "        # (bs, n_enc_seq, d_hidn), (bs, d_hidn), [(bs, n_head, n_enc_seq, n_enc_seq)]\n",
    "        outputs, outputs_cls, attn_probs = self.bert(inputs, segments)\n",
    "        # (bs, 2)\n",
    "        logits_cls = self.projection_cls(outputs_cls)\n",
    "        # (bs, n_enc_seq, n_enc_vocab)\n",
    "        logits_lm = self.projection_lm(outputs)\n",
    "        # (bs, n_enc_vocab), (bs, n_enc_seq, n_enc_vocab), [(bs, n_head, n_enc_seq, n_enc_seq)]\n",
    "        return logits_cls, logits_lm, attn_probs\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrain Data 생성\n",
    "[MASK] 생성함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 마스크 생성 \"\"\"\n",
    "from random import randrange, shuffle\n",
    "import random \n",
    "\n",
    "def create_pretrain_mask(tokens, mask_cnt, vocab_list):\n",
    "    cand_idx = []\n",
    "    # 1) token을 단어별로 index 배열 형태로 저장\n",
    "    for (i, token) in enumerate(tokens):\n",
    "        if token == \"[CLS]\" or token == \"[SEP]\":\n",
    "            continue\n",
    "        if 0 < len(cand_idx) and not token.startswith(u\"\\u2581\"): # u\"\\u2581\" : 단어의 시작을 의미\n",
    "            cand_idx[-1].append(i) \n",
    "        else:\n",
    "            cand_idx.append([i])\n",
    "    # 2) random 선택을 위해 단어의 index 섞음\n",
    "    shuffle(cand_idx) \n",
    "\n",
    "    mask_lms = []\n",
    "    # 3) mask_lms의 개수가 mask_cnt를 넘지 않도록 함 (mask_cnt는 전체 token의 15%)\n",
    "    for index_set in cand_idx: \n",
    "        if len(mask_lms) >= mask_cnt:\n",
    "            break\n",
    "        if len(mask_lms) + len(index_set) > mask_cnt:\n",
    "            continue\n",
    "        for index in index_set:\n",
    "            masked_token = None\n",
    "            if random.random() < 0.8: # 80% 마스킹\n",
    "                masked_token = \"[MASK]\"\n",
    "            else:\n",
    "                if random.random() < 0.5: # 10% 는 original로 유지\n",
    "                    masked_token = tokens[index]\n",
    "                else: # 10%는 random으로 vocab_list에서 임의의 값 선택\n",
    "                    masked_token = random.choice(vocab_list)\n",
    "            # 4) mask된 index값과 정답 label을 maSK_lms에 저장\n",
    "            mask_lms.append({\"index\": index, \"label\": tokens[index]})\n",
    "            # 5) token index의 값을 mask\n",
    "            tokens[index] = masked_token\n",
    "    # 6) Random하게 mask된 값을 index순으로 정렬\n",
    "    mask_lms = sorted(mask_lms, key=lambda x: x[\"index\"])\n",
    "    # 7) 정렬된 값을 이용해 mask_idx와 mask_label을 생성\n",
    "    mask_idx = [p[\"index\"] for p in mask_lms]\n",
    "    mask_label = [p[\"label\"] for p in mask_lms]\n",
    "\n",
    "    return tokens, mask_idx, mask_label    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 최대 길이 초과하는 토큰 자르기\n",
    "1) token A의 길이가 길 경우 앞에서부터 토큰을 제거 (line: 11~12)\n",
    "2) token B의 길이가 길 경우 뒤에서부터 토큰을 제거 (line: 13~14)\n",
    " \"\"\"\n",
    "def trim_tokens(tokens_a, tokens_b, max_seq):\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_seq:\n",
    "            break\n",
    "\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            del tokens_a[0]\n",
    "        else:\n",
    "            tokens_b.pop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단락별 pretrain 데이터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" doc 별 pretrain 데이터 생성 \"\"\"\n",
    "def create_pretrain_instances(docs, doc_idx, doc, n_seq, mask_prob, vocab_list):\n",
    "    # for [CLS], [SEP], [SEP]\n",
    "    max_seq = n_seq - 3 # 특수 토큰 총 3개\n",
    "    tgt_seq = max_seq\n",
    "\n",
    "    instances = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    for i in range(len(doc)):\n",
    "        current_chunk.append(doc[i]) # 단락을 줄 단위로 돌며 current_chunk에 line 추가\n",
    "        current_length += len(doc[i]) # current_length에 line의 token수를 더함\n",
    "        if i == len(doc) - 1 or current_length >= tgt_seq: # 마지막 line이거나, current_length가 tgt_seq를 넘을 경우 학습데이터 만듦\n",
    "            if 0 < len(current_chunk): # \n",
    "                a_end = 1\n",
    "                if 1 < len(current_chunk):\n",
    "                    a_end = randrange(1, len(current_chunk)) # randrange(1, len(current_chunk)) : 1~len(current_chunk) 중 랜덤값 반환\n",
    "                tokens_a = []\n",
    "                for j  in range(a_end): # token 수 랜덤으로 선택해서 tokens_a에 추가\n",
    "                    tokens_a.extend(current_chunk[j])\n",
    "                \n",
    "                tokens_b = []\n",
    "                # 50%의 확률로 다른 단락에서 tokens_b 만듦\n",
    "                if len(current_chunk) == 1 or random.random() < 0.5: \n",
    "                    is_next = 0 # is_next의 값은 false (0)\n",
    "                    tokens_b_len = tgt_seq - len(tokens_a) \n",
    "                    random_doc_idx = doc_idx\n",
    "                    while doc_idx == random_doc_idx:\n",
    "                        random_doc_idx = randrange(0, len(docs))\n",
    "                    random_doc = docs[random_doc_idx]\n",
    "\n",
    "                    random_start = randrange(0, len(random_doc))\n",
    "                    for j in range(random_start, len(random_doc)):\n",
    "                        tokens_b.extend(random_doc[j])\n",
    "                # 50%의 확률로 current_chunk에서 tokens_a 이후부터 tokens_b 만듦\n",
    "                else:\n",
    "                    is_next = 1 #is_next의 값은 true (1)\n",
    "                    for j in range(a_end, len(current_chunk)):\n",
    "                        tokens_b.extend(current_chunk[j])\n",
    "\n",
    "                trim_tokens(tokens_a, tokens_b, max_seq) # token 크기 줄이기\n",
    "                assert 0 < len(tokens_a) # 조건이 참이 아니면 AssertionError 발생\n",
    "                assert 0 < len(tokens_b)\n",
    "\n",
    "                tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"] + tokens_b + [\"[SEP]\"] # '[CLS]'+ tokens_a + '[SEP]' + tokens_b + '[SEP]' 형태로 tokens 생성\n",
    "                segment = [0] * (len(tokens_a) + 2) + [1] * (len(tokens_b) + 1) # tokens_a의 길이 + 2 만큼 0으로, tokens_b의 길이 + 1 만큼 1로 segment 생성\n",
    "\n",
    "                tokens, mask_idx, mask_label = create_pretrain_mask(tokens, int((len(tokens)-3)*mask_prob), vocab_list) # mask 생성 (전체 token 수의 15% 만큼 mask)\n",
    "\n",
    "                instance = {\n",
    "                    \"tokens\": tokens,\n",
    "                    \"segment\": segment,\n",
    "                    \"is_next\": is_next,\n",
    "                    \"mask_idx\": mask_idx,\n",
    "                    \"mask_label\": mask_label\n",
    "                }\n",
    "                instances.append(instance)\n",
    "\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "    return instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrain 데이터 생성 함수 정의\n",
    "- 말뭉치를 읽어 pretrain data 만듦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\"\"\" pretrain 데이터 생성 \"\"\"\n",
    "def make_pretrain_data(vocab, in_file, out_file, count, n_seq, mask_prob):\n",
    "    vocab_list = [] # 단어목록 생성\n",
    "    for id in range(vocab.get_piece_size()): \n",
    "        if not vocab.is_unknown(id): # 생성 시, unknown은 제거\n",
    "            vocab_list.append(vocab.id_to_piece(id))\n",
    "\n",
    "    line_cnt = 0\n",
    "    with open(in_file, \"r\") as in_f:\n",
    "        for line in in_f:\n",
    "            line_cnt += 1 # line 수 count\n",
    "\n",
    "    docs = []\n",
    "    with open(in_file, \"r\") as f:\n",
    "        doc = []\n",
    "        with tqdm(total=line_cnt, desc=f\"Loading\") as pbar:\n",
    "            for i, line in enumerate(f):\n",
    "                line = line.strip()\n",
    "                if line == \"\":\n",
    "                    if 0 < len(doc):\n",
    "                        docs.append(doc)\n",
    "                        doc = []\n",
    "                else:\n",
    "                    pieces = vocab.encode_as_pieces(line)\n",
    "                    if 0 < len(pieces):\n",
    "                        doc.append(pieces)\n",
    "                pbar.update(1)\n",
    "\n",
    "        if doc:\n",
    "            docs.append(doc)\n",
    "\n",
    "    for index in range(count):\n",
    "        output = out_file.format(index)\n",
    "        if os.path.isfile(output):\n",
    "            continue\n",
    "        with open(output, \"w\") as out_f:\n",
    "            with tqdm(total=len(docs), desc=f\"Making\") as pbar:\n",
    "                for i, doc in enumerate(docs):\n",
    "                    instances = create_pretrain_instances(docs, i, doc, n_seq, mask_prob, vocab_list)\n",
    "                    for instance in instances:\n",
    "                        out_f.write(json.dumps(instance)) # python의 객체를 json 문자열로 변환 (json = JavaScript Object Notation)\n",
    "                        out_f.write(\"\\n\")\n",
    "                    pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrain 데이터 생성 실행\n",
    "**Settings**\n",
    "- 말뭉치 개수(count) = 10\n",
    "- sequence 길이 (n_seq) = 256\n",
    "- Mask 확률 (mask_prob) = 15 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading:  39%|███▉      | 2058360/5234475 [02:09<02:22, 22295.86it/s]"
     ]
    }
   ],
   "source": [
    "in_file = \"/opt/workspace/Seohyeon/Projects/LMMs/data_preprocessing/DATA/kowiki.txt\"\n",
    "out_file = \"/opt/workspace/Seohyeon/Projects/LMMs/data_preprocessing/DATA/kowiki_bert_{}.json\"\n",
    "count = 10 \n",
    "n_seq = 256\n",
    "mask_prob = 0.15\n",
    "\n",
    "make_pretrain_data(vocab, in_file, out_file, count, n_seq, mask_prob)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrain DataSet 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\"\"\" pretrain 데이터셋 \"\"\"\n",
    "class PretrainDataSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, vocab, infile):\n",
    "        self.vocab = vocab\n",
    "        self.labels_cls = []\n",
    "        self.labels_lm = []\n",
    "        self.sentences = []\n",
    "        self.segments = []\n",
    "\n",
    "        line_cnt = 0\n",
    "        with open(infile, \"r\") as f:\n",
    "            for line in f:\n",
    "                line_cnt += 1\n",
    "        \n",
    "        with open(infile, \"r\") as f:\n",
    "            for i, line in enumerate(tqdm(f, total=line_cnt, desc=f\"Loading {infile}\", unit=\"lines\")):\n",
    "                instance = json.loads(line)\n",
    "                self.labels_cls.append(instance[\"is_next\"]) # tokens_a와 tokens_b가 인접한 문장인지 여부\n",
    "                sentences = [vocab.piece_to_id(p) for p in instance[\"tokens\"]] # token을 id(숫자)로 변환\n",
    "                self.sentences.append(sentences)\n",
    "                self.segments.append(instance[\"segment\"]) # tokens_a(0)와  tokens_b(1)를 구분하기 위한 값\n",
    "                mask_idx = np.array(instance[\"mask_idx\"], dtype=np.int) # mask_idx: tokens내의 mask index\n",
    "                mask_label = np.array([vocab.piece_to_id(p) for p in instance[\"mask_label\"]], dtype=np.int) # mask_label: tokens내의 mask된 부분의 정답\n",
    "                label_lm = np.full(len(sentences), dtype=np.int, fill_value=-1) # 값이 모두 -1인 label_lm 변수 생성\n",
    "                label_lm[mask_idx] = mask_label # label_lm의  mask_idx 위치에 mask_label 값 저장 => mask_idx 위치는 mask_label이 나머지는 -1이 됨\n",
    "                self.labels_lm.append(label_lm) \n",
    "\n",
    "    def __len__(self):\n",
    "        assert len(self.labels_cls) == len(self.labels_lm)\n",
    "        assert len(self.labels_cls) == len(self.sentences)\n",
    "        assert len(self.labels_cls) == len(self.segments)\n",
    "        return len(self.labels_cls)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        return (torch.tensor(self.labels_cls[item]),\n",
    "                torch.tensor(self.labels_lm[item]),\n",
    "                torch.tensor(self.sentences[item]),\n",
    "                torch.tensor(self.segments[item]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 배치단위로 데이터 처리하기 위한 collate_fn 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" pretrain data collate_fn \"\"\"\n",
    "def pretrain_collate_fn(inputs):\n",
    "    labels_cls, labels_lm, inputs, segments = list(zip(*inputs))\n",
    "\n",
    "    # labels_lm의 길이가 같아지도록 짧은 문장에 padding(-1) 추가\n",
    "    labels_lm = torch.nn.utils.rnn.pad_sequence(labels_lm, batch_first=True, padding_value=-1) # pad_sequence(): stacks a list of Tensors along a new dimension, and pads them to equal length\n",
    "    # inputs의 길이가 같아지도록 짧은 문장에 padding(0) 추가\n",
    "    inputs = torch.nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=0)\n",
    "    # segments의 길이가 같아지도록 짧은 문장에 padding(0) 추가\n",
    "    segments = torch.nn.utils.rnn.pad_sequence(segments, batch_first=True, padding_value=0)\n",
    "\n",
    "    batch = [\n",
    "        torch.stack(labels_cls, dim=0), # labels_cls는 길이가 1고정이므로 stack함수를 이용해 tensor로 만듦\n",
    "        labels_lm,\n",
    "        inputs,\n",
    "        segments\n",
    "    ]\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader\n",
    "- 위에서 정의한 DataSet과 collate_fn을 이용해 학습용 (train_loader) DataLoader 생성\n",
    "- 위에서 생성한 pretrain data중 첫 번째 값을 읽음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading C:/PRML/personal_project/LLMs/data_preprocessing/DATA/kowiki_bert_0.json:   0%|          | 0/1042228 [00:00<?, ?lines/s]C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_16884\\3505543699.py:23: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  mask_idx = np.array(instance[\"mask_idx\"], dtype=np.int) # mask_idx: tokens내의 mask index\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_16884\\3505543699.py:24: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  mask_label = np.array([vocab.piece_to_id(p) for p in instance[\"mask_label\"]], dtype=np.int) # mask_label: tokens내의 mask된 부분의 정답\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_16884\\3505543699.py:25: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  label_lm = np.full(len(sentences), dtype=np.int, fill_value=-1) # 값이 모두 -1인 label_lm 변수 생성\n",
      "Loading C:/PRML/personal_project/LLMs/data_preprocessing/DATA/kowiki_bert_0.json: 100%|██████████| 1042228/1042228 [05:00<00:00, 3467.47lines/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" pretrain 데이터 로더 \"\"\"\n",
    "batch_size = 128\n",
    "dataset = PretrainDataSet(vocab, \"/opt/workspace/Seohyeon/Projects/LMMs/data_preprocessing/DATA/kowiki_bert_0.json\")\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=pretrain_collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrain\n",
    "**Train**\n",
    "- BERT model을 pretrain하기 위한 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 모델 epoch 학습 \"\"\"\n",
    "def train_epoch(config, epoch, model, criterion_lm, criterion_cls, optimizer, train_loader):\n",
    "    losses = []\n",
    "    model.train()\n",
    "\n",
    "    with tqdm(total=len(train_loader), desc=f\"Train({epoch})\") as pbar:\n",
    "        for i, value in enumerate(train_loader):\n",
    "            labels_cls, labels_lm, inputs, segments = map(lambda v: v.to(config.device), value)\n",
    "\n",
    "            labels_lm = labels_lm.long()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs, segments)\n",
    "            logits_cls, logits_lm = outputs[0], outputs[1] # NSP(logit_cls), MLM(logit_lm) 반환\n",
    "\n",
    "            # A 문장 뒤 B 문장인지 T/F 분류하는 loss\n",
    "            loss_cls = criterion_cls(logits_cls, labels_cls)\n",
    "            # Mask된 token을 예측하는 loss\n",
    "            loss_lm = criterion_lm(logits_lm.view(-1, logits_lm.size(2)), labels_lm.view(-1)) # (bs, seq_len, vocab_size) -> (bs*seq_len, vocab_size) \n",
    "                                                                                              # 각 토큰에 대한 모델의 예측(각 단어의 logit 값) 비교\n",
    "            loss = loss_cls + loss_lm\n",
    "\n",
    "            loss_val = loss_lm.item()\n",
    "            losses.append(loss_val)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix_str(f\"Loss: {loss_val:.3f} ({np.mean(losses):.3f})\")\n",
    "            \n",
    "\n",
    "    return np.mean(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_enc_vocab': 8007, 'n_enc_seq': 256, 'n_seg_type': 2, 'n_layer': 6, 'd_hidn': 256, 'i_pad': 0, 'd_ff': 1024, 'n_head': 4, 'd_head': 64, 'dropout': 0.1, 'layer_norm_epsilon': 1e-12, 'device': device(type='cuda', index=0)}\n"
     ]
    }
   ],
   "source": [
    "config.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(config)\n",
    "\n",
    "learning_rate = 5e-5\n",
    "n_epoch = 20\n",
    "count = 10 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main: Operating code for training BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load pretrain from: C:/PRML/personal_project/LLMs/BERT/pretrained_BERT/save_best_model.pth, epoch=0, loss=1.9920589530471646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train(1): 100%|██████████| 32570/32570 [1:08:21<00:00,  7.94it/s, Loss: 0.794 (1.283)]\n",
      "Loading C:/PRML/personal_project/LLMs/data_preprocessing/DATA/kowiki_bert_2.json:   0%|          | 0/1042228 [00:00<?, ?lines/s]C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_16884\\3505543699.py:23: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  mask_idx = np.array(instance[\"mask_idx\"], dtype=np.int) # mask_idx: tokens내의 mask index\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_16884\\3505543699.py:24: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  mask_label = np.array([vocab.piece_to_id(p) for p in instance[\"mask_label\"]], dtype=np.int) # mask_label: tokens내의 mask된 부분의 정답\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_16884\\3505543699.py:25: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  label_lm = np.full(len(sentences), dtype=np.int, fill_value=-1) # 값이 모두 -1인 label_lm 변수 생성\n",
      "Loading C:/PRML/personal_project/LLMs/data_preprocessing/DATA/kowiki_bert_2.json: 100%|██████████| 1042228/1042228 [07:25<00:00, 2340.39lines/s]\n",
      "Train(2): 100%|██████████| 32570/32570 [1:10:38<00:00,  7.69it/s, Loss: 0.873 (0.912)]\n",
      "Loading C:/PRML/personal_project/LLMs/data_preprocessing/DATA/kowiki_bert_3.json: 100%|██████████| 1042228/1042228 [10:21<00:00, 1677.41lines/s]\n",
      "Train(3):  33%|███▎      | 10651/32570 [23:36<48:35,  7.52it/s, Loss: 1.041 (0.845)]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [21], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m PretrainDataSet(vocab, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/PRML/personal_project/LLMs/data_preprocessing/DATA/kowiki_bert_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m%\u001b[39m\u001b[38;5;250m \u001b[39mcount\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     26\u001b[0m     train_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, collate_fn\u001b[38;5;241m=\u001b[39mpretrain_collate_fn)\n\u001b[1;32m---> 28\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion_lm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion_cls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss)\n\u001b[0;32m     30\u001b[0m model\u001b[38;5;241m.\u001b[39mbert\u001b[38;5;241m.\u001b[39msave(epoch, loss, save_pretrain)\n",
      "Cell \u001b[1;32mIn [19], line 23\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(config, epoch, model, criterion_lm, criterion_cls, optimizer, train_loader)\u001b[0m\n\u001b[0;32m     20\u001b[0m                                                                                   \u001b[38;5;66;03m# 각 토큰에 대한 모델의 예측(각 단어의 logit 값) 비교\u001b[39;00m\n\u001b[0;32m     21\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_cls \u001b[38;5;241m+\u001b[39m loss_lm\n\u001b[1;32m---> 23\u001b[0m loss_val \u001b[38;5;241m=\u001b[39m \u001b[43mloss_lm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss_val)\n\u001b[0;32m     26\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "# 모델 선언\n",
    "model = BERTPretrain(config)\n",
    "\n",
    "save_pretrain = \"/opt/workspace/Seohyeon/Projects/LMMs/BERT/pretrained_BERT/save_best_model.pth\"\n",
    "best_epoch, best_loss = 0, 0\n",
    "# 기존에 학습된 pretrain값이 있으면 로드\n",
    "if os.path.isfile(save_pretrain):\n",
    "    best_epoch, best_loss = model.bert.load(save_pretrain)\n",
    "    print(f\"load pretrain from: {save_pretrain}, epoch={best_epoch}, loss={best_loss}\")\n",
    "    best_epoch += 1\n",
    "\n",
    "model.to(config.device)\n",
    "\n",
    "criterion_lm = torch.nn.CrossEntropyLoss(ignore_index=-1, reduction='mean')\n",
    "criterion_cls = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "losses = []\n",
    "offset = best_epoch\n",
    "for step in range(n_epoch):\n",
    "    epoch = step + offset\n",
    "    if 0 < step:\n",
    "        del train_loader\n",
    "        dataset = PretrainDataSet(vocab, f\"/opt/workspace/Seohyeon/Projects/LMMs/data_preprocessing/DATA/kowiki_bert_{epoch % count}.json\")\n",
    "        train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=pretrain_collate_fn)\n",
    "    \n",
    "    loss = train_epoch(config, epoch, model, criterion_lm, criterion_cls, optimizer, train_loader)\n",
    "    losses.append(loss)\n",
    "    model.bert.save(epoch, loss, save_pretrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "# data\n",
    "data = {\n",
    "    \"loss\": losses\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "display(df)\n",
    "\n",
    "# graph\n",
    "plt.figure(figsize=[8, 4])\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Depth')\n",
    "plt.xlim((0, n_epoch - 1))\n",
    "plt.ylabel('Position')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Naver 영화리뷰 감정분석 분류 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" naver movie classfication \"\"\"\n",
    "class MovieClassification(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.bert = BERT(self.config)\n",
    "        # classfier\n",
    "        self.projection_cls = nn.Linear(self.config.d_hidn, self.config.n_output, bias=False)\n",
    "    \n",
    "    def forward(self, inputs, segments):\n",
    "        # (bs, n_enc_seq, d_hidn), (bs, d_hidn), [(bs, n_head, n_enc_seq, n_enc_seq)]\n",
    "        outputs, outputs_cls, attn_probs = self.bert(inputs, segments)\n",
    "        # (bs, n_output)\n",
    "        logits_cls = self.projection_cls(outputs_cls)\n",
    "        # (bs, n_output), [(bs, n_head, n_enc_seq, n_enc_seq)]\n",
    "        return logits_cls, attn_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 영화 분류 데이터셋 \"\"\"\n",
    "class MovieDataSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, vocab, infile):\n",
    "        self.vocab = vocab\n",
    "        self.labels = []\n",
    "        self.sentences = []\n",
    "        self.segments = []\n",
    "\n",
    "        line_cnt = 0\n",
    "        with open(infile, \"r\") as f:\n",
    "            for line in f:\n",
    "                line_cnt += 1\n",
    "\n",
    "        with open(infile, \"r\") as f:\n",
    "            for i, line in enumerate(tqdm(f, total=line_cnt, desc=\"Loading Dataset\", unit=\" lines\")):\n",
    "                data = json.loads(line)\n",
    "                self.labels.append(data[\"label\"])\n",
    "                sentence = [vocab.piece_to_id(\"[CLS]\")] + [vocab.piece_to_id(p) for p in data[\"doc\"]] + [vocab.piece_to_id(\"[SEP]\")]\n",
    "                self.sentences.append(sentence)\n",
    "                self.segments.append([0] * len(sentence))\n",
    "    \n",
    "    def __len__(self):\n",
    "        assert len(self.labels) == len(self.sentences)\n",
    "        assert len(self.labels) == len(self.segments)\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        return (torch.tensor(self.labels[item]),\n",
    "                torch.tensor(self.sentences[item]),\n",
    "                torch.tensor(self.segments[item]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" movie data collate_fn \"\"\"\n",
    "def movie_collate_fn(inputs):\n",
    "    labels, inputs, segments = list(zip(*inputs))\n",
    "\n",
    "    inputs = torch.nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=0)\n",
    "    segments = torch.nn.utils.rnn.pad_sequence(segments, batch_first=True, padding_value=0)\n",
    "\n",
    "    batch = [\n",
    "        torch.stack(labels, dim=0),\n",
    "        inputs,\n",
    "        segments,\n",
    "    ]\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 데이터 로더 \"\"\"\n",
    "batch_size = 128\n",
    "train_dataset = MovieDataSet(vocab, \"/opt/workspace/Seohyeon/Projects/LMMs/data_preprocessing/DATA/ratings_train.json\")\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=movie_collate_fn)\n",
    "test_dataset = MovieDataSet(vocab, \"/opt/workspace/Seohyeon/Projects/LMMs/data_preprocessing/DATA/ratings_test.json\")\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=movie_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 모델 epoch 평가 \"\"\"\n",
    "def eval_epoch(config, model, data_loader):\n",
    "    matchs = []\n",
    "    model.eval()\n",
    "\n",
    "    n_word_total = 0\n",
    "    n_correct_total = 0\n",
    "    with tqdm(total=len(data_loader), desc=f\"Valid\") as pbar:\n",
    "        for i, value in enumerate(data_loader):\n",
    "            labels, inputs, segments = map(lambda v: v.to(config.device), value)\n",
    "\n",
    "            outputs = model(inputs, segments)\n",
    "            logits_cls = outputs[0]\n",
    "            _, indices = logits_cls.max(1)\n",
    "\n",
    "            match = torch.eq(indices, labels).detach()\n",
    "            matchs.extend(match.cpu())\n",
    "            accuracy = np.sum(matchs) / len(matchs) if 0 < len(matchs) else 0\n",
    "\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix_str(f\"Acc: {accuracy:.3f}\")\n",
    "    return np.sum(matchs) / len(matchs) if 0 < len(matchs) else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 모델 epoch 학습 \"\"\"\n",
    "def train_epoch(config, epoch, model, criterion_cls, optimizer, train_loader):\n",
    "    losses = []\n",
    "    model.train()\n",
    "\n",
    "    with tqdm(total=len(train_loader), desc=f\"Train({epoch})\") as pbar:\n",
    "        for i, value in enumerate(train_loader):\n",
    "            labels, inputs, segments = map(lambda v: v.to(config.device), value)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs, segments)\n",
    "            logits_cls = outputs[0]\n",
    "\n",
    "            loss_cls = criterion_cls(logits_cls, labels)\n",
    "            loss = loss_cls\n",
    "\n",
    "            loss_val = loss_cls.item()\n",
    "            losses.append(loss_val)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix_str(f\"Loss: {loss_val:.3f} ({np.mean(losses):.3f})\")\n",
    "    return np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "config.n_output = 2\n",
    "print(config)\n",
    "\n",
    "learning_rate = 5e-5\n",
    "n_epoch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model):\n",
    "    model.to(config.device)\n",
    "\n",
    "    criterion_cls = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    best_epoch, best_loss, best_score = 0, 0, 0\n",
    "    losses, scores = [], []\n",
    "    for epoch in range(n_epoch):\n",
    "        loss = train_epoch(config, epoch, model, criterion_cls, optimizer, train_loader)\n",
    "        score = eval_epoch(config, model, test_loader)\n",
    "\n",
    "        losses.append(loss)\n",
    "        scores.append(score)\n",
    "\n",
    "        if best_score < score:\n",
    "            best_epoch, best_loss, best_score = epoch, loss, score\n",
    "    print(f\">>>> epoch={best_epoch}, loss={best_loss:.5f}, socre={best_score:.5f}\")\n",
    "    return losses, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train (No pretrain)\n",
    "- MovieClassification 생성 -> 추가적인 처리 없이 생성된 MovieClassification으로 학습 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MovieClassification(config)\n",
    "\n",
    "losses_00, scores_00 = train(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train (20 epoch Pretrain)\n",
    "- 20 epoch Pretrain된 모델을 이용해 학습 진행\n",
    "- MovieClassification 생성 -> Pretrain모델 로드 -> MovieClassification으로 학습 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MovieClassification(config)\n",
    "\n",
    "save_pretrain = \"/opt/workspace/Seohyeon/Projects/LMMs/BERT/pretrained_BERT/save_best_model.pth\"\n",
    "model.bert.load(save_pretrain)\n",
    "\n",
    "losses_20, scores_20 = train(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "- Pretrain 안한 경우와 한 경우의 정확도 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table\n",
    "data = {\n",
    "    \"loss_00\": losses_00,\n",
    "    \"socre_00\": scores_00,\n",
    "    \"loss_20\": losses_20,\n",
    "    \"socre_20\": scores_20,\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "display(df)\n",
    "\n",
    "# graph\n",
    "plt.figure(figsize=[12, 4])\n",
    "plt.plot(scores_00, label=\"score_00\")\n",
    "plt.plot(scores_20, label=\"score_20\")\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Value')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
